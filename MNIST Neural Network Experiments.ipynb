{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f46cdb7-3653-417b-a4c3-41604c08dcd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4aa945bd-481b-4884-a880-644a84b15476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1000)>\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9.91M/9.91M [00:01<00:00, 5.42MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1000)>\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28.9k/28.9k [00:00<00:00, 1.51MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1000)>\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1.65M/1.65M [00:00<00:00, 7.42MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1000)>\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4.54k/4.54k [00:00<?, ?B/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Define transformation\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "# Load training and test sets\n",
    "train_dataset = MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4468cbbe-69f2-406a-bd2e-68bc0e45b6c8",
   "metadata": {},
   "source": [
    "# 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f78c5dd3-8563-4967-a562-d1045d97024a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaselineModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BaselineModel, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(28*28, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Initialize model, loss, and optimizer\n",
    "model = BaselineModel()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c87c74e-fc47-47c8-99eb-c2c13b691938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 1.0660\n",
      "Epoch [2/5], Loss: 0.3901\n",
      "Epoch [3/5], Loss: 0.3255\n",
      "Epoch [4/5], Loss: 0.2914\n",
      "Epoch [5/5], Loss: 0.2659\n"
     ]
    }
   ],
   "source": [
    "def train_model(model, optimizer, criterion, num_epochs=5):\n",
    "    start_time = time.time()\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for images, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}\")\n",
    "    \n",
    "    exec_time = time.time() - start_time\n",
    "    return exec_time\n",
    "\n",
    "# Train baseline model and calculate execution time\n",
    "baseline_exec_time = train_model(model, optimizer, criterion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7bc1704-82f6-4eb9-97ef-2e33e294e80f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Model Accuracy: 92.78%, Execution Time: 30.89 seconds\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy\n",
    "\n",
    "# Evaluate baseline model\n",
    "baseline_accuracy = evaluate_model(model)\n",
    "print(f\"Baseline Model Accuracy: {baseline_accuracy:.2f}%, Execution Time: {baseline_exec_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3815169-1eec-4ed9-b501-0c4b955d13ab",
   "metadata": {},
   "source": [
    "# 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "348d415d-bd61-4344-a4f2-76782fe27fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define CustomActivationModel\n",
    "class CustomActivationModel(nn.Module):\n",
    "    def __init__(self, activation_fn):\n",
    "        super(CustomActivationModel, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(28*28, 128)  # Assuming input images are 28x28, like in the MNIST dataset\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)      # Output layer for a 10-class classification problem\n",
    "        self.activation_fn = activation_fn\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.activation_fn(self.fc1(x))  # Applies the custom activation function to the first layer\n",
    "        x = self.activation_fn(self.fc2(x))  # Applies the custom activation function to the second layer\n",
    "        x = self.fc3(x)                      # No activation function on the output layer (e.g., for classification tasks)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bc4fb0c7-3840-46d3-8015-e27e27fc9f07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 0.8168\n",
      "Epoch [2/5], Loss: 0.3669\n",
      "Epoch [3/5], Loss: 0.3285\n",
      "Epoch [4/5], Loss: 0.3131\n",
      "Epoch [5/5], Loss: 0.3042\n",
      "Activation Function: Identity, Accuracy: 91.72%, Execution Time: 29.47 seconds\n",
      "Epoch [1/5], Loss: 1.0329\n",
      "Epoch [2/5], Loss: 0.3847\n",
      "Epoch [3/5], Loss: 0.3233\n",
      "Epoch [4/5], Loss: 0.2915\n",
      "Epoch [5/5], Loss: 0.2675\n",
      "Activation Function: ReLU, Accuracy: 92.37%, Execution Time: 29.51 seconds\n",
      "Epoch [1/5], Loss: 1.0227\n",
      "Epoch [2/5], Loss: 0.4168\n",
      "Epoch [3/5], Loss: 0.3323\n",
      "Epoch [4/5], Loss: 0.2932\n",
      "Epoch [5/5], Loss: 0.2665\n",
      "Activation Function: Tanh, Accuracy: 92.60%, Execution Time: 29.37 seconds\n",
      "\n",
      "Summary of Results:\n",
      "Activation: Identity, Accuracy: 91.72%, Execution Time: 29.47 seconds\n",
      "Activation: ReLU, Accuracy: 92.37%, Execution Time: 29.51 seconds\n",
      "Activation: Tanh, Accuracy: 92.60%, Execution Time: 29.37 seconds\n"
     ]
    }
   ],
   "source": [
    "# Define the activation functions: one linear and two non-linear\n",
    "activations = {\n",
    "    'Identity': lambda x: x,  # Linear activation function\n",
    "    'ReLU': torch.relu,       # Non-linear activation function\n",
    "    'Tanh': torch.tanh        # Non-linear activation function\n",
    "}\n",
    "\n",
    "# Dictionary to store the results\n",
    "results = []\n",
    "\n",
    "# Loop through each activation function\n",
    "for name, activation_fn in activations.items():\n",
    "    # Initialize the model with the specific activation function\n",
    "    model = CustomActivationModel(activation_fn)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "    \n",
    "    # Measure training time\n",
    "    start_time = time.time()\n",
    "    exec_time = train_model(model, optimizer, criterion)\n",
    "    end_time = time.time()\n",
    "    training_duration = end_time - start_time\n",
    "    \n",
    "    # Evaluate the model to get accuracy\n",
    "    accuracy = evaluate_model(model)\n",
    "    \n",
    "    # Store results\n",
    "    results.append({\n",
    "        \"Activation Function\": name,\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Execution Time\": training_duration\n",
    "    })\n",
    "    \n",
    "    # Print the results for this activation function\n",
    "    print(f\"Activation Function: {name}, Accuracy: {accuracy:.2f}%, Execution Time: {training_duration:.2f} seconds\")\n",
    "\n",
    "# Final Results\n",
    "print(\"\\nSummary of Results:\")\n",
    "for result in results:\n",
    "    print(f\"Activation: {result['Activation Function']}, Accuracy: {result['Accuracy']:.2f}%, Execution Time: {result['Execution Time']:.2f} seconds\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24896c2-dcdb-4ee4-8421-f4cc2c784b06",
   "metadata": {},
   "source": [
    "# 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0eef58f1-5cae-4da6-ac32-4359156dfaa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 0.9902\n",
      "Epoch [2/5], Loss: 0.3823\n",
      "Epoch [3/5], Loss: 0.3236\n",
      "Epoch [4/5], Loss: 0.2884\n",
      "Epoch [5/5], Loss: 0.2623\n",
      "Optimizer: SGD, Accuracy: 92.33%, Execution Time: 30.71 seconds\n",
      "Epoch [1/5], Loss: 0.4109\n",
      "Epoch [2/5], Loss: 0.2005\n",
      "Epoch [3/5], Loss: 0.1468\n",
      "Epoch [4/5], Loss: 0.1151\n",
      "Epoch [5/5], Loss: 0.0995\n",
      "Optimizer: Adam, Accuracy: 96.83%, Execution Time: 43.21 seconds\n",
      "Epoch [1/5], Loss: 0.3818\n",
      "Epoch [2/5], Loss: 0.1864\n",
      "Epoch [3/5], Loss: 0.1368\n",
      "Epoch [4/5], Loss: 0.1102\n",
      "Epoch [5/5], Loss: 0.0923\n",
      "Optimizer: RMSprop, Accuracy: 97.16%, Execution Time: 33.95 seconds\n",
      "\n",
      "Summary of Results:\n",
      "Optimizer: SGD, Accuracy: 92.33%, Execution Time: 30.71 seconds\n",
      "Optimizer: Adam, Accuracy: 96.83%, Execution Time: 43.21 seconds\n",
      "Optimizer: RMSprop, Accuracy: 97.16%, Execution Time: 33.95 seconds\n"
     ]
    }
   ],
   "source": [
    "# Define the optimizers\n",
    "optimizers = {\n",
    "    'SGD': lambda params: optim.SGD(params, lr=0.01),\n",
    "    'Adam': lambda params: optim.Adam(params, lr=0.001),\n",
    "    'RMSprop': lambda params: optim.RMSprop(params, lr=0.001)\n",
    "}\n",
    "\n",
    "# Dictionary to store the results\n",
    "results = []\n",
    "\n",
    "# Loop through each optimizer\n",
    "for name, optimizer_fn in optimizers.items():\n",
    "    # Initialize the model and the optimizer for each experiment\n",
    "    model = CustomActivationModel(torch.relu)  # Using ReLU as the activation function for consistency\n",
    "    optimizer = optimizer_fn(model.parameters())\n",
    "    \n",
    "    # Measure training time\n",
    "    start_time = time.time()\n",
    "    exec_time = train_model(model, optimizer, criterion)\n",
    "    end_time = time.time()\n",
    "    training_duration = end_time - start_time\n",
    "    \n",
    "    # Evaluate the model to get accuracy\n",
    "    accuracy = evaluate_model(model)\n",
    "    \n",
    "    # Store results\n",
    "    results.append({\n",
    "        \"Optimizer\": name,\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Execution Time\": training_duration\n",
    "    })\n",
    "    \n",
    "    # Print the results for this optimizer\n",
    "    print(f\"Optimizer: {name}, Accuracy: {accuracy:.2f}%, Execution Time: {training_duration:.2f} seconds\")\n",
    "\n",
    "# Final Results Summary\n",
    "print(\"\\nSummary of Results:\")\n",
    "for result in results:\n",
    "    print(f\"Optimizer: {result['Optimizer']}, Accuracy: {result['Accuracy']:.2f}%, Execution Time: {result['Execution Time']:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74c67b9-0d7e-4894-9650-4a64c88c71db",
   "metadata": {},
   "source": [
    "# 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "52f5850c-044c-4059-a39d-25ef1e3d3881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 1.5487\n",
      "Epoch [2/5], Loss: 0.7958\n",
      "Epoch [3/5], Loss: 0.6195\n",
      "Epoch [4/5], Loss: 0.5351\n",
      "Epoch [5/5], Loss: 0.4880\n",
      "Model: Dropout, Accuracy: 92.36%, Execution Time: 30.98 seconds\n",
      "Epoch [1/5], Loss: 0.5722\n",
      "Epoch [2/5], Loss: 0.2195\n",
      "Epoch [3/5], Loss: 0.1557\n",
      "Epoch [4/5], Loss: 0.1210\n",
      "Epoch [5/5], Loss: 0.0996\n",
      "Model: BatchNorm, Accuracy: 97.36%, Execution Time: 29.36 seconds\n",
      "Epoch [1/5], Loss: 0.5805\n",
      "Epoch [2/5], Loss: 0.3061\n",
      "Epoch [3/5], Loss: 0.2560\n",
      "Epoch [4/5], Loss: 0.2244\n",
      "Epoch [5/5], Loss: 0.2018\n",
      "Model: WeightInit, Accuracy: 94.48%, Execution Time: 30.62 seconds\n",
      "\n",
      "Summary of Results:\n",
      "Model: Dropout, Accuracy: 92.36%, Execution Time: 30.98 seconds\n",
      "Model: BatchNorm, Accuracy: 97.36%, Execution Time: 29.36 seconds\n",
      "Model: WeightInit, Accuracy: 94.48%, Execution Time: 30.62 seconds\n"
     ]
    }
   ],
   "source": [
    "# Define the Dropout model\n",
    "class DropoutModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DropoutModel, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(28*28, 128)\n",
    "        self.dropout1 = nn.Dropout(0.5)  # Dropout with 50% rate\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Define the BatchNorm model\n",
    "class BatchNormModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BatchNormModel, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(28*28, 128)\n",
    "        self.bn1 = nn.BatchNorm1d(128)  # BatchNorm after first layer\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = torch.relu(self.bn1(self.fc1(x)))\n",
    "        x = torch.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Define the Weight Initialization model\n",
    "class WeightInitModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(WeightInitModel, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(28*28, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "        \n",
    "        # Apply Xavier (Glorot) initialization\n",
    "        self._initialize_weights()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        init.xavier_uniform_(self.fc1.weight)\n",
    "        init.xavier_uniform_(self.fc2.weight)\n",
    "        init.xavier_uniform_(self.fc3.weight)\n",
    "\n",
    "# Dictionary to store the results\n",
    "results = []\n",
    "\n",
    "# Define models to test\n",
    "models = {\n",
    "    'Dropout': DropoutModel(),\n",
    "    'BatchNorm': BatchNormModel(),\n",
    "    'WeightInit': WeightInitModel()\n",
    "}\n",
    "\n",
    "# Loop through each model\n",
    "for name, model in models.items():\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "    \n",
    "    # Measure training time\n",
    "    start_time = time.time()\n",
    "    exec_time = train_model(model, optimizer, criterion)\n",
    "    end_time = time.time()\n",
    "    training_duration = end_time - start_time\n",
    "    \n",
    "    # Evaluate the model to get accuracy\n",
    "    accuracy = evaluate_model(model)\n",
    "    \n",
    "    # Store results\n",
    "    results.append({\n",
    "        \"Model\": name,\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Execution Time\": training_duration\n",
    "    })\n",
    "    \n",
    "    # Print the results for this model\n",
    "    print(f\"Model: {name}, Accuracy: {accuracy:.2f}%, Execution Time: {training_duration:.2f} seconds\")\n",
    "\n",
    "# Final Results Summary\n",
    "print(\"\\nSummary of Results:\")\n",
    "for result in results:\n",
    "    print(f\"Model: {result['Model']}, Accuracy: {result['Accuracy']:.2f}%, Execution Time: {result['Execution Time']:.2f} seconds\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfadbbd-a8c4-4853-a6b0-459cc7da9a3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
